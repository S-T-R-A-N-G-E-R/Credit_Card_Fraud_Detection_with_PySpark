{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7764c1d",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection with PySpark\n",
    "Step-by-step setup: data loading, EDA, preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ae7c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/09/16 08:24:24 WARN Utils: Your hostname, Swapnils-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.38 instead (on interface en0)\n",
      "25/09/16 08:24:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/09/16 08:24:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetection\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdef502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/09/16 08:24:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionEDA\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reduce Spark log noise\n",
    "spark.sparkContext.setLogLevel(\"FATAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77598171",
   "metadata": {},
   "source": [
    "## Load CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094ead61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train transaction rows: 590540\n",
      "Train identity rows: 144233\n",
      "Test transaction rows: 506691\n",
      "Test identity rows: 141907\n"
     ]
    }
   ],
   "source": [
    "train_transaction_path = \"../data/raw/train_transaction.csv\"\n",
    "train_identity_path    = \"../data/raw/train_identity.csv\"\n",
    "test_transaction_path  = \"../data/raw/test_transaction.csv\"\n",
    "test_identity_path     = \"../data/raw/test_identity.csv\"\n",
    "\n",
    "train_transaction_df = spark.read.csv(train_transaction_path, header=True, inferSchema=True)\n",
    "train_identity_df    = spark.read.csv(train_identity_path, header=True, inferSchema=True)\n",
    "test_transaction_df  = spark.read.csv(test_transaction_path, header=True, inferSchema=True)\n",
    "test_identity_df     = spark.read.csv(test_identity_path, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Train transaction rows:\", train_transaction_df.count())\n",
    "print(\"Train identity rows:\", train_identity_df.count())\n",
    "print(\"Test transaction rows:\", test_transaction_df.count())\n",
    "print(\"Test identity rows:\", test_identity_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a399d1",
   "metadata": {},
   "source": [
    "## Join transactions with identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d700a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined train rows: 590540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined test rows: 506691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df = train_transaction_df.join(train_identity_df, on=\"TransactionID\", how=\"left\")\n",
    "test_df  = test_transaction_df.join(test_identity_df, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "print(\"Joined train rows:\", train_df.count())\n",
    "print(\"Joined test rows:\", test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ef792",
   "metadata": {},
   "source": [
    "## Basic EDA: Class balance, missing values, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c838b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|isFraud| count|\n",
      "+-------+------+\n",
      "|      1| 20663|\n",
      "|      0|569877|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|    TransactionAmt|\n",
      "+-------+------------------+\n",
      "|  count|            590540|\n",
      "|   mean|135.02717637246874|\n",
      "| stddev| 239.1625220137336|\n",
      "|    min|             0.251|\n",
      "|    max|         31937.391|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fraud distribution\n",
    "train_df.groupBy(\"isFraud\").count().show()\n",
    "\n",
    "# Example: Transaction amount stats\n",
    "train_df.select(\"TransactionAmt\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f019c",
   "metadata": {},
   "source": [
    "## Standardize and Align Columns (fix train vs test mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2810b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Standardized and aligned: Train=433, Test=433\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def standardize_and_align(train_df: DataFrame, test_df: DataFrame):\n",
    "    # Replace '-' with '_' in both dfs\n",
    "    def clean_columns(df):\n",
    "        for old_col in df.columns:\n",
    "            new_col = old_col.replace(\"-\", \"_\").strip()\n",
    "            if old_col != new_col:\n",
    "                df = df.withColumnRenamed(old_col, new_col)\n",
    "        return df\n",
    "\n",
    "    train_df = clean_columns(train_df)\n",
    "    test_df  = clean_columns(test_df)\n",
    "\n",
    "    # Align schemas\n",
    "    common_cols = sorted(list(set(train_df.columns).intersection(set(test_df.columns))))\n",
    "    train_df = train_df.select(common_cols)\n",
    "    test_df  = test_df.select(common_cols)\n",
    "\n",
    "    print(f'✅ Standardized and aligned: Train={len(train_df.columns)}, Test={len(test_df.columns)}')\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df_clean, test_df_clean = standardize_and_align(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f145bc",
   "metadata": {},
   "source": [
    "## Drop High-Missing Columns (>90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cbacdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 12 columns with >90% missing values\n",
      "Train shape (cols): 421\n",
      "Test shape (cols): 421\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_count = train_df_clean.count()\n",
    "missing_dict = {c: train_df_clean.filter(col(c).isNull()).count()/total_count for c in train_df_clean.columns}\n",
    "\n",
    "drop_cols = [c for c, miss in missing_dict.items() if miss > 0.9]\n",
    "train_df_clean = train_df_clean.drop(*drop_cols)\n",
    "test_df_clean  = test_df_clean.drop(*drop_cols)\n",
    "\n",
    "print(f'Dropped {len(drop_cols)} columns with >90% missing values')\n",
    "print('Train shape (cols):', len(train_df_clean.columns))\n",
    "print('Test shape (cols):', len(test_df_clean.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82507853",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e280519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1901:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |isFraud|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|(2818,[0,2,4,5,6,10,13,14,15,16,20,21,22,23,24,26,27,28,29,30,39,40,41,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,64,74,75,76,84,85,87,88,89,90,91,92,93,94,95,109,110,115,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,166,167,173,174,175,176,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,242,243,244,252,259,269,298,299,300,301,303,305,308,309,310,311,314,319,320,321,325,327,328,331,332,333,336,342,343,344,345,347,350,351,354,355,356,358,369,370,371,372,373,374,375,376,377,384,385,386,387,388,389,390,2179,2181,2183,2185,2189,2191,2199,2257,2323,2326],[2.0,1.0,12.0,2.0,2.0,3.0,3.0,61.0,40.0,302.0,318.0,61.0,30.0,318.0,30.0,37.75,0.6666659712791443,117.0,86536.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,495.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,204.0,87.0,17399.0,111.0,150.0,224.0,19.0,-5.0,125672.0,100.0,52.0,-300.0,166.0,341.0,472.0,24.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                     |0      |\n",
      "|(2818,[0,2,4,5,6,9,10,13,16,21,22,24,26,27,28,29,30,31,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,64,74,75,76,84,85,87,88,89,90,91,92,93,94,95,109,110,115,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,166,167,173,174,175,176,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,233,234,241,242,243,244,252,253,259,298,299,300,301,303,305,308,309,310,311,312,313,314,318,319,320,321,325,327,328,331,332,333,335,336,337,342,343,344,345,347,350,351,354,355,356,358,359,360,369,370,371,372,373,374,375,376,377,384,385,386,387,388,389,390,2179,2181,2184,2190,2192,2195,2196,2198,2257,2322,2326],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,237.0,97.0,8.0,10.0,37.75,0.6666659712791443,47.95,86725.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,184.0,87.0,4663.0,490.0,150.0,166.0,5.0,-5.0,125672.0,100.0,52.0,-300.0,166.0,341.0,472.0,24.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|0      |\n",
      "|(2818,[0,2,4,5,6,9,10,13,21,22,24,26,27,28,29,30,31,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,64,74,75,76,84,85,87,88,89,90,91,92,93,94,95,109,110,115,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,166,167,173,174,175,176,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,233,234,241,242,243,244,252,253,259,298,299,300,301,303,305,308,309,310,311,312,313,314,318,319,320,321,325,327,328,331,332,333,335,336,337,342,343,344,345,347,350,351,354,355,356,358,359,360,369,370,371,372,373,374,375,376,377,384,385,386,387,388,389,390,2179,2181,2183,2190,2198,2257,2323,2326],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,97.0,8.0,10.0,37.75,0.6666659712791443,107.95,86808.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,126.0,87.0,2392.0,360.0,150.0,166.0,4.0,-5.0,125672.0,100.0,52.0,-300.0,166.0,341.0,472.0,24.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                   |0      |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# --- Identify numeric and categorical columns ---\n",
    "numeric_cols = [\n",
    "    f.name for f in train_df_clean.schema.fields \n",
    "    if f.dataType.simpleString() in [\"double\", \"int\"] and f.name not in [\"isFraud\", \"TransactionID\"]\n",
    "]\n",
    "categorical_cols = [\n",
    "    f.name for f in train_df_clean.schema.fields \n",
    "    if f.dataType.simpleString() == \"string\" and f.name != \"TransactionID\"\n",
    "]\n",
    "\n",
    "# --- Impute numerics ---\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCols=[f\"{c}_imputed\" for c in numeric_cols]\n",
    ").setStrategy(\"median\")\n",
    "\n",
    "# --- Encode categoricals ---\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") \n",
    "    for c in categorical_cols\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\") \n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# --- Assemble feature vector ---\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_imputed\" for c in numeric_cols] + [f\"{c}_ohe\" for c in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# --- Build pipeline ---\n",
    "pipeline = Pipeline(stages=[imputer] + indexers + encoders + [assembler])\n",
    "\n",
    "# --- Fit + transform ---\n",
    "pipeline_model = pipeline.fit(train_df_clean)\n",
    "train_prepared = pipeline_model.transform(train_df_clean)\n",
    "test_prepared  = pipeline_model.transform(test_df_clean)\n",
    "\n",
    "print(\"✅ Preprocessing complete\")\n",
    "\n",
    "# --- Preserve labels ---\n",
    "# Ensure labels are still attached\n",
    "train_labels = train_df.select(\"TransactionID\", \"isFraud\")  # original df before dropping\n",
    "train_prepared = train_prepared.join(train_labels, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "# --- Check sample ---\n",
    "train_prepared.select(\"features\", \"isFraud\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e987f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2017:=================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logistic Regression Validation Results: AUC = 0.8580, F1 = 0.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "# --- Step 1: Train/Validation Split ---\n",
    "train_split, val_split = train_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# --- Step 2: Define Logistic Regression ---\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"isFraud\",\n",
    "    maxIter=10,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "# --- Step 3: Fit model ---\n",
    "lr_model = lr.fit(train_split)\n",
    "\n",
    "# --- Step 4: Predictions on validation set ---\n",
    "val_predictions = lr_model.transform(val_split)\n",
    "\n",
    "# --- Step 5: Evaluation ---\n",
    "# AUC (ROC)\n",
    "auc_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = auc_eval.evaluate(val_predictions)\n",
    "\n",
    "# F1-score\n",
    "f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1 = f1_eval.evaluate(val_predictions)\n",
    "\n",
    "print(f\"✅ Logistic Regression Validation Results: AUC = {auc:.4f}, F1 = {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd8e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2231:=================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Random Forest Results (Train Validation): AUC = 0.8613, F1 = 0.9646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# --- Step 8B: Random Forest Classifier ---\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"isFraud\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    subsamplingRate=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train RF on train_prepared\n",
    "rf_model = rf.fit(train_prepared)\n",
    "\n",
    "# Predict on train_prepared (validation split would be better, but this keeps it simple for now)\n",
    "rf_predictions = rf_model.transform(train_prepared)\n",
    "\n",
    "# --- Evaluation ---\n",
    "auc_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc_rf = auc_eval.evaluate(rf_predictions)\n",
    "\n",
    "f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_rf = f1_eval.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"✅ Random Forest Results (Train Validation): AUC = {auc_rf:.4f}, F1 = {f1_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6eb5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
